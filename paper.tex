\documentclass[11pt,letterpaper]{article}
\usepackage{emnlp2016}
\usepackage[utf8]{inputenc}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
%\emnlpfinalcopy

%  Enter the EMNLP Paper ID here:
\def\emnlppaperid{***}

%\setlength\titlebox{5cm}

% How about this:
\title{Levels of representation in a Stacked Gated Recurrent Neural
  Network model of visually-grounded language learning}
\author{Lieke Gelderloos \\
  Tilburg University \\
  {\tt l.j.gelderloos@uvt.nl} \And
  Grzegorz Chrupa?a \\
  Tilburg University \\
  {\tt g.chrupala@uvt.nl} }

\date{}

\begin{document}
\maketitle
\begin{abstract}
Language learning children discover both the structure of language and the ways in which it relates to the world from continuous perceptual data in different modalities. We present {Phoneme GRU}, an architecture composed of stacked Gated Recurrent Units that learns to predict visual features given an image caption in the form of a sequence of phonemes. Although the language input consists of low-level symbolic input rather than a continuous speech signal, the learning task resembles that human language learners in the sense that the learner learns both structure and meaning from noisy and ambiguous data. We show that {\sc Phoneme GRU} indeed learns to predict features of the visual context given phonetically transcribed image captions, and show that it represented linguistic structure in a hierarchical manner: lower layers in the stack are comparatively more sensitive to form, whereas higher layers are more sensitive to meaning.

\end{abstract}

\input{intro.tex}
\input{related.tex}
\input{models.tex}
\input{experiments.tex}
\input{discussion.tex}



\bibliographystyle{emnlp2016}
\bibliography{biblio}

\end{document}