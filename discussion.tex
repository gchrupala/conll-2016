\section{Discussion}
\label{sec:discussion}
In this paper we have shown that a model of stacked Gated Recurrent Units is capable of learning to extract visually significant aspects of meaning from sequential phoneme data. The {\sc Stacked GRU} model was trained to predict a high level image feature vector from phonetically transcribed captions. On the training image retrieval task, {\sc Stacked GRU} was outperformed by a single hidden layer GRU model with a word embedding layer that takes words as input. However, accuracy of {\sc Stacked GRU} on the training task was higher than that of {\sc Word Sum}, the analog of a bag of words model. This indicates that although taking phoneme-level instead of word-level data as input makes the training task more difficult, the GRU-architecture does allow {\sc Stacked GRU} to exploit sentence order information.
We explored the role of each of the layers in the stack of hidden units in {\sc Stacked GRU} in representing form as well as meaning. A word boundary prediction experiment indicated that the lowest layer is most involved in encoding information about word form, as its activation pattern provided a mre accurate predictor of word boundaries than that of the second, and even more so for the third layer. A word similarity experiment on the MEN dataset showed that cosine similarity at any hidden layer and edit distance of two word pairs were negatively correlated. This correlation is strongest in the lowest hidden layer and decreases in magnitude for the higher layers. 
Human judgements of semantic relatedness were correlated most strongly with cosine similarities between the highest hidden layer, less strongly with cosine similarities of the middle hidden layer, and weakest in the lowest hidden layer, indicating that semantic information is encoded mostly in the higher layers. On the word similarity judgement task, the cosine distances between activation patterns of the hidden layer of both word-based models were correlated more strongly with human similarity judgements than those of {\sc Stacked GRU}. This may be due to the fact that these models have a word embedding layer, and most likely is also helped by the fact that these models do not have the additional task of recognizing a word as such. [suggest replacement experiment here]
As we go up in the stack of hidden layers, the timescale on which the layer operates increases. Sentences that have similar activation vectors at the end of sentence in the highest hidden layer have shared sequences at positions earlier in the sentence than sentences that are similar in the second, and certainly than in the first layer. These findings are consistent with \newcite{hermans2013training}. It shows the input data is processed hierarchically through time, with the lowest layer capturing structure over short sequences, such as words, and the higher levels capturing structure over longer stretches, such as sentences. 

[suggest that it may be interesting to investigate the role of word form more thoroughly, perhaps with regards to morphology, or when encountering unknown words]