\section{Discussion}
\label{sec:discussion}
In this paper we have shown that a model of stacked Gated Recurrent Units is capable of learning to extract visually significant aspects of meaning from sequential phoneme data. The {\sc Phoneme GRU} model was trained to predict a high level image feature vector from phonetically transcribed captions. On the training image retrieval task, {\sc Phoneme GRU} was outperformed by a single hidden layer GRU model with a word embedding layer that takes words as input. However, accuracy of {\sc Stacked GRU} on the training task was higher than that of {\sc Word Sum}, the analog of a bag of words model. This indicates that although taking phoneme-level instead of word-level data as input makes the training task more difficult, the GRU-architecture does allow {\sc Phoneme GRU} to exploit sentence order information.

We explored the role of each of the layers in the stack of hidden units in {\sc Phoneme GRU} in representing form as well as meaning. A word boundary prediction experiment indicated that the lowest layer is most involved in encoding information about word form, as its activation pattern provided a more accurate predictor of word boundaries than that of the second and third layer. A word similarity experiment on the MEN dataset showed that cosine similarity at any hidden layer and edit distance of two word pairs were negatively correlated. This correlation is strongest in the lowest hidden layer and decreases in magnitude for the higher layers, again indicating that information about word form is represented low in the stack of GRU's.

Human judgements of semantic relatedness of word pairs, on the other hand, were correlated most strongly with cosine similarities between activation vectors of the highest hidden layer, and decreasingly so for the lower layers. This indicates that semantic information is encoded mostly in the higher layers. On the word similarity judgement task, cosine similarities between activation patterns of the hidden layer of both word-based models were correlated more strongly with human similarity judgements than those of any layer of {\sc Phoneme GRU}. A contributing reason for this difference may be the word embedding layer that is present in the word based models, but not in {\sc Phoneme GRU}. In addition, the word based models have the advantage that there is no ambiguity between a word form and a word, as there is for {\sc Phoneme GRU}. The fact that the correlations between human word similarity ratings and cosine similarities of activation vectors in {\sc Phoneme GRU} are stronger for frequent words, suggests that this disambiguation is nevertheless learnable.

As we go up in the stack of hidden layers, the timescale on which the layer operates increases. Sentences that have similar activation vectors at the end of sentence in the highest hidden layer have shared sequences at positions earlier in the sentence than sentences that are similar in the second, and certainly than in the first layer. These findings are consistent with findings in \newcite{hermans2013training}. It shows the input data is processed hierarchically through time, with the lowest layer capturing structure over short sequences, such as words, and the higher levels capturing structure over longer stretches, such as sentences. 

The hierarchical representation of linguistic structure, and the hierarchical processing through time, is not absolutely separated between the layers. Although there is a clear pattern of short-timescale information in the lower layers and larger dependencies in the higher layers, the third layer still encodes information about the phonetic form; its activation patterns were predictive of word boundaries, and similarities between word pairs at this level were more strongly correlated with edit distance than human similarty judgements are. It would be interesting to investigate exactly what information that is. Could it be that sub-word structural information that is still present in the third layer is semantically relevant, or visually salient? 
[CLOSING PARAGRAPH]