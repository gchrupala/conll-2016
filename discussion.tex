\section{Discussion}
\label{sec:discussion}
In this paper we have shown that a model of stacked Gated Recurrent Units is capable of learning to extract visually significant aspects of meaning from sequential phoneme data. The {\sc Stacked GRU} model was trained to predict a high level image feature vector from phonetically transcribed captions. On the training image retrieval task, {\sc Stacked GRU} was outperformed by a single hidden layer GRU model with a word embedding layer that takes words as input. However, accuracy of{\sc Stacked GRU} on the training task was higher than that of {\sc Word Sum}, the analog of a bag of words model. This indicates that although taking phoneme-level instead of word-level data as input makes the training task more difficult, the GRU-architecture does allow{\sc Stacked GRU} to exploit sentence order information.
We explored the role of each of the layers in the stack of hidden units in {\sc Stacked GRU} in relation to different levels of representation. A word boundary prediction experiment indicated that the lower layers are more involved in encoding information about phonemic form. A word similarity experiment on the MEN dataset showed that cosine similarity of activation patterns of word pairs were mostly . These findings are consistent with the findings of \newcite{hermans2013training}. 
Human similarity judgements of word pairs were correlated most strongly with cosine similarities between the highest hidden layer, less strongly with cosine similarities of the middle hidden layer, and weakest in the lowest hidden layer, indicating that semantic information is encoded mostly in the higher layers. On the word similarity judgement task, the cosine distances between activation patterns of the hidden layer of both word-based models were correlated more strongly with human similarity judgements than those of {\sc Stacked GRU}. This may be due to the fact that these models have a word embedding layer, and most likely is also helped by the fact that these models do not have the additional task of recognizing a word as such. [suggest replacement experiment here]
As we go up in the stack of hidden layers, the timescale on which the layer operates increases. Sentences that have similar activation vectors at the end of sentence in the highest hidden layer have shared sequences at positions earlier in the sentence than sentences that are similar in the second, and certainly than in the first layer. [relate to hermans & Schraauwen?] This supports the idea that information about form is processed in the lower layers, whereas [not only emerges in higher layers but also integrates over longer stretches of time]
[suggest that it may be interesting to investigate the role of word form more thoroughly, perhaps with regards to morphology, or when encountering unknown words]