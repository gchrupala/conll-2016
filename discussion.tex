\section{Discussion}
\label{sec:discussion}
We trained a the {\sc Phon GRU} model  to predict high level image features  from phonetically transcribed captions. On the training image retrieval task, {\sc Phon GRU} was outperformed by a the word-level {\sc Word GRU}. However, accuracy of {\sc Phon GRU} on the training task was higher than that of {\sc Word Sum}, the analog of a bag of words model. This indicates that although taking phoneme-level instead of word-level data as input makes the training task more difficult, the GRU-architecture does allow {\sc Phon GRU} to exploit sequential ordering information.

We explored the role of each of the layers in the stack of hidden layers in {\sc Phon GRU} in representing form as well as meaning. A word boundary prediction experiment indicated that the lowest layer is most involved in encoding information about word form, as its activation pattern provided the most accurate predictor of word boundaries. A word similarity experiment on the MEN dataset showed that cosine similarity at any hidden layer and edit distance of two word pairs were negatively correlated. This correlation is strongest in the lowest hidden layer, again indicating that information about word form is represented low in the stack of GRUs.

Human judgements of semantic relatedness of word pairs, on the other hand, were correlated most strongly with cosine similarities between activation vectors of the top hidden layer, and decreasingly so for the lower layers. This indicates that semantic information is encoded mostly in the higher layers. On the word similarity judgement task, cosine similarities between activation patterns of the hidden layer of both word-based models were correlated more strongly with human similarity judgements than those of any layer of {\sc Phon GRU}. A contributing reason for this difference may be the word embedding layer that is present in the word based models, but not in {\sc Phon GRU}. In addition, the word based models have the advantage that there is no ambiguity between a word form and a word, as there is for {\sc Phon GRU}. The fact that the correlations between human word similarity ratings and cosine similarities of activation vectors in {\sc Phon GRU} are stronger for frequent words, suggests that this disambiguation is nevertheless learnable.

As we go up in the stack of hidden layers, the timescale on which the layer operates increases. Sentences that have similar activation vectors at the end of sentence in the highest hidden layer have shared sequences at positions earlier in the sentence than sentences that are similar in the second, and certainly than in the first layer. These findings are consistent with findings in \newcite{hermans2013training}. It shows the input data is processed hierarchically through time, with the lowest layer capturing structure over short sequences, such as words, and the higher levels capturing structure over longer stretches, such as sentences. 

\section{Conclusion and future work}
The hierarchical representation of linguistic structure, and the hierarchical processing through time, is not absolutely separated between the layers. Although there is a clear pattern of short-timescale information in the lower layers and larger dependencies in the higher layers, the third layer still encodes information about the phonetic form: its activation patterns were predictive of word boundaries, and similarities between word pairs at this level were more strongly correlated with edit distance than human similarity judgments are. It would be interesting to investigate exactly what information that is, and to what extent it is analogous to language representation in the mind of human speakers. For example, in humans both word phonological form and word meaning can act as primes, which hints that human representations encode both aspects, which is somewhat reminiscent of the behavior of  our model.

Finally, we would like to take the next step towards grounded learning of language from raw perceptual input, and apply models similar to the one described here to acoustic speech signal coupled with visual input. We expect this to be a challenging but essential endeavor.

%\section*{Acknowledgements}
