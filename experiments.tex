\section{Experiments}
\label{sec:experiments}

% introduceer experimenten?
For all experiments, the model was trained on the training split of the MSCOCO dataset. Captions were transcribed phonetically automatically using eSpeak. Stress and pause markers were cleaned out, as well as word boundaries, leaving only the phonemic symbols. Visual input consisted of % ja wat eigenlijk. misschien moet dit wel in de models-sectie?

\subsection{Word boundary prediction}
To explore the sensitivity of the model to linguistic structure at the sub-word level, we trained logistic regression models to predict and recognize word boundaries from the activation patterns of all hidden layers. The task of the prediction models was to determine whether a certain timestep was a word end, given the activation pattern at one of the hidden layers. The task of the recognition models was to identify word onsets based on activation patterns. Data was shuffled before training and testing. All models were trained on a random selection of 5000 sentences from the validation split, and tested on the rest of the validation split. % model settings %are these standardized splits? By whom? how much is 'the rest'? was it a good idea to randomly select sentences, rather than pictures?
For comparison, we also trained logistic regression models on n-gram data to perform the same tasks. The features available to these models were positional n-grams. %write more once you know what you're doing

\subsection{Word Similarity} % you may want to say semantic, although that is somewhat questionable, and relatedness rather than similarity...
The semantic validity of the model was evaluated on the MEN dataset. {} %cite 
For each word pair in the MEN dataset, the words were transcribed phonetically automatically and then fed to the model individually. Cosine similarity between the activation patterns of the third hidden layer at the last timestep was taken as the measure of similarity. % did you take the last timestep or end-of-sentence-symbol? If last timstep, why?
Since the model has access to the surface forms of words, it may be able to exploit phonetic similarity when determining semantic relatedness. This may especially be the case when encountering infrequent words. If the meaning of a word is not known from previous encounters, well-known words that are similar in form may provide some clue to its meaning. To determine whether such an effect of phonetic similarity exists, we included a measure of phonemic difference, as well as the frequency of words in the training portion of MSCOCO. The measure of phonetic difference was the edit distance between the phonetic transcriptions of the two words, normalized by dividing it by the length of the longest transcription.

%the role of phonetic similarity...

%Compared all words to each other. Lev distance is explanatory for ...... Also do this as a function of the frequency of both words. I suspect when one word is well known and the other is not, lev will be explanatory. However, if both are unknown, not so much, although they will be similar in that case (because they are both similar to some third, well-known word). If they are both well known, lev distance may not matter.

\subsection{Word similarity: replacement}
The experiment previously described takes cosine similarity between the activations of the last hidden layer as a measure of word similarity, in analogy to how similarity of word embeddings is commonly quantified. However, the last hidden layer of our model is very different to an embedding layer. Its task is to predict visual features based on a whole sentence, rather than a single word. This experiment, then, was designed to measure word similarity in a way that better suits the functionality of the model % wat bedoel je hiermee.
The input data were sentences from the validation part of the MSCOCO dataset that contained either one of the two words of each word pair in the MEN dataset. In each of these sentences % niet waar, je neemt er max 10 . also de laatste zin was onontwarbaar
the word that got it selected, was replaced by its counterpart from the MEN dataset. The resulting sentence was transcribed phonetically. Similarity of the sentences was measured by taking the cosine similarity between the activations at the end-of-sentence symbol for the original sentence and the sentence in which the original word had been replaced. Per word pair, the similarity of the sentence pairs was averaged, which was taken as the similarity score. % something about this perhaps not being simmetric?
