\section{Experiments}
\label{sec:experiments}

% introduceer experimenten?
For all experiments, the model was trained on the training split of the MSCOCO dataset. Captions were transcribed phonetically automatically using eSpeak. Stress and pause markers were cleaned out, as well as word boundaries, leaving only the phonemic symbols. % Visual input consisted of  ja wat eigenlijk. misschien moet dit wel in de models-sectie?

\subsection{Word boundary prediction}
To explore the sensitivity of the model to linguistic structure at the sub-word level, we trained logistic regression models to predict and recognize word boundaries from the activation patterns of all hidden layers. The task of the prediction models was to determine whether a certain timestep was a word end, given the activation pattern at one of the hidden layers. The task of the recognition models was to identify word onsets based on activation patterns. Data was shuffled before training and testing. All models were trained on a random selection of 5000 sentences from the validation split, and tested on the rest of the validation split. % model settings %are these standardized splits? (no) By whom? how much is 'the rest'? was it a good idea to randomly select sentences, rather than pictures?
For comparison, we also trained logistic regression models on n-gram data to perform the same tasks. The features available to these models were positional n-grams. %write more once you know what you're doing

\subsection{Word Similarity} % you may want to say semantic, although that is somewhat questionable, and relatedness rather than similarity...
To understand the encoding of semantic information by the model, we compared similarity of activation vectors to human similarity judgements from the MEN dataset. %cite 
For each word pair in the MEN dataset, the words were transcribed phonetically automatically and then fed to the model individually. Cosine similarity between the activation patterns of the third hidden layer at the last timestep was taken as the measure of similarity. % did you take the last timestep or end-of-sentence-symbol? If last timstep, why?
Since the model has access to the surface forms of words, it may be able to exploit surface similarity when determining semantic relatedness. This may especially be the case when encountering infrequent words. If the meaning of a word is not known from previous encounters, well-known words that are similar in form may provide some clue to its meaning. To determine whether such an effect of phonetic similarity exists, a measure of phonemic difference was included: the edit distance between the phonetic transcriptions of the two words, normalized by dividing it by the length of the longest transcription.

\subsection{Word similarity: replacement}
The experiment previously described takes cosine similarity between the activations of the last hidden layer as a measure of word similarity, in analogy to how similarity of word embeddings is commonly quantified. However, the last hidden layer of our model is very different to an embedding layer. Its task is to integrate information over a whole sentence, as a basis for predicting features of the visual scene it describes. In the current experiment, the difference between two words is how much the prediction of the visual scene changes when one word replaces the other. % niet waar je pakt de laatste hidden layer, niet de uiteindelijke voorspelling
The input data were sentences from the validation part %je hebt nu train-part! OPNIEUW 
of the MSCOCO dataset that contained either one of the two words of each word pair in the MEN dataset. In each of these sentences % niet waar, je neemt er max 20. also de laatste zin was onontwarbaar
the word that got it selected, was replaced by its counterpart from the MEN dataset. The resulting sentence was transcribed phonetically. Similarity of the sentences was measured by taking the cosine similarity between the activation vector of the highest hidden layer at the end-of-sentence symbol for the original sentence and the sentence in which the original word had been replaced. The similarity score of a word pair was the average similarity of the sentence pairs. % something about this perhaps not being simmetric? 
