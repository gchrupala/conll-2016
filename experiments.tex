\section{Experiments}
\label{sec:experiments}

For all experiments, the models were trained on the training set of MS COCO. Textual input was transcribed phonetically automatically using the default English voice of eSpeak for use with the {\sc Phoneme GRU} models. Stress and pause markers were cleaned out, as well as word boundaries (after storing their position for use in experiments), leaving only phoneme symbols. Visual input for all models consists of activation vectors of the pre-softmax layer of the 16-layer CNN described in \newcite{simonyan2014very}.
\input{visual}
\subsection{Word boundary prediction}
To explore the sensitivity of the {\sc Phoneme GRU} model to linguistic structure at the sub-word level, we investigated the encoding of information about word-ends in the hidden layers. A logistic regression model was trained on activation patterns of the hidden layers at all timesteps, with the objective of identifying phonemes that preceded a word boundary. For comparison, we also trained logistic regression models on \textit{n}-gram data to perform the same tasks. The features available to these models were positional phoneme \textit{n}-grams in the range 1 - \textit{n}. Both the \textit{n}-gram models and hidden state models were implemented using scikitlearn implementation with l2-regularization and C set to 1000. The location of the word boundaries was taken from the eSpeak transcriptions, which mostly matches the actual location of word boundaries. However, eSpeak models some coarticulation effects that sometimes leads to word boundaries disappearing from the transcription.

[example of a phonetic transcription with 'ova']

Accuracy scores reported in table 1 are the averages over 5-fold cross validation on the validation portion of MS COCO. The proportion of phonemes preceding a word boundary is .29, meaning that predicting 'no word boundary' by default would be correct in .71 of cases. At the highest hidden layer, enough information about the word form is available to correctly predict word end in .82 of cases - an improvement of .11 compared to the null baseline. The lower levels allow for more accurate prediction of word boundaries: .86 at the middle hidden layer, and .88 at the lowest level. These results indicate that information on sub-word structure is mostly encoded in the lower layers, and is not as present in the higher layers. %Lelijke zin
Prediction accuracy of the logistic regression model based on the activation patterns of the lowest hidden layer is comparable to that of a bigram logistic regression model.

\begin{table}[]
	\centering
	\begin{tabular}{ccc}
		Model & & precision \\
		\hline
		Activation vector & Layer 1 & 0.88 \\
		& Layer 2 & 0.86 \\
		& Layer 3 & 0.82 \\
		\hline
		\textit{n}-gram & \textit{n} = 1 & 0.79 \\
		& \textit{n} = 2 & 0.88 \\
		& \textit{n} = 3 & 0.93 \\
		& \textit{n} = 4 & 0.95
	\end{tabular}
	\caption{Precision of word boundary prediction}
\end{table}

\subsection{Word similarity}
To understand the encoding of semantic information in {\sc Stacked GRU}, we analysed the cosine similarity of activation vectors for word pairs from the MEN dataset, and compared them to human similarity judgements.
For each word pair in the MEN dataset, the words were transcribed phonetically using eSpeak and then fed to {\sc Stacked GRU} individually. For comparison, the words were also fed to {\sc Word GRU} and {\sc Word Sum}. Word pair similarity was quantified as the cosine similarity between the activation patterns of the hidden layers at the end-of-sentence symbol.
In contrast to {\sc Word GRU} and {\sc Word Sum}, {\sc Stacked GRU} has access to the word forms. To explore the role of phonemic form in word similarity, a measure of phonemic difference was included: the Levenshtein distance between the phonetic transcriptions of the two words, normalized by dividing it by the length of the longer transcription. 

Table 2 shows Spearman's rank correlation coefficient between human similarity ratings from the MEN dataset and cosine similarity at the last timestep for all hidden layers. In all layers, the cosine similarities between the activation vectors for two words are significantly correlated with human similarity judgements. The strength of the correlation differs considerably between the layers, ranging from .09 in the first layer to 0.28 in the highest hidden layer. The second column in Table 2 shows the correlations when only taking into account the 1283 word pairs of which both words appear at least 100 times in the training data. 
Correlations for both {\sc Word GRU} and {\sc Word SUM} are considerably higher than for {\sc Stacked GRU}.

\begin{table}[]
	\centering
	\begin{tabular}{ccc}
		& All word pairs & Pairs of frequent words \\
		{\sc Phoneme GRU} 
		\multicolumn{1}{|r|}{Layer 1} & 0.09 & 0.12\\
		\multicolumn{1}{|r|}{Layer 2} & 0.21 & 0.33 \\
		\multicolumn{1}{|r|}{Layer 3}& 0.28 & 0.45 \\
		\hline
		{\sc Word GRU} & 0.48 & 0.60\\
		\hline
		{\sc Word Sum} & 0.42 & 0.56
	\end{tabular}
	\caption{Spearman's rank correlation coefficient between cosine similarity and human similarity judgements. All correlations significant at \textit{p} < 0.05.} % bad title
\end{table}

Table 3 shows Spearman's rank correlation coefficient between the edit distance and the cosine similarity of activation vectors at the hidden layers of {\sc Stacked GRU}.
As is to be expected, edit distance and cosine similarity of the activation vectors are negatively correlated, which means that words that are more similar in form are also more similar in meaning. Note that in the MEN dataset, meaning and word form are indeed correlated: human similarity judgements and edit distance are correlated at -0.08 (\textit{p} < 0.05). The correlation is strongest at the lowest hidden layer and weakest, though still present and stronger than for human judgements, at the third hidden layer. 
The correlations of cosine similarities with edit distance on the one hand, and human similarity rating on the other hand, indicate that the different hidden layers reflect increasing levels of representation: whereas at the lowest level mostly encodes information about form, the highest layer mostly encodes semantic information.

\begin{table}[]
	\centering
	\begin{tabular}{cc}
		Layer 1 & -0.30 \\
		Layer 2 & -0.24 \\
		Layer 3 & -0.15
	\end{tabular}
	\caption{Spearman's rank correlation coefficient between cosine similarity and edit distance. All correlations significant at \textit{p} < 0.05.} % bad title
\end{table}

\subsection{Shared sequences}
In this experiment we looked at the positions of phoneme strings that were shared by sentences from the validation set of MS COCO and their nearest neighbours. We determined each sentences' nearest neighbour for each hidden layer in {\sc Stacked GRU}. The nearest neighbour is the sentence for which the activation vector at the end of sentence symbol has the smallest cosine distance to the activation vector of the original sentence. The position of matching subsequences is the average position in the original sentence of characters in substrings that are shared by the neighbour sentences, counted from the end of the sentence. A high mean average substring position thus means that the shared substring(s) appear early in the sentence. This gives an indirect measure of the timescale at which the different layers operate. 
As can be read from Table 4, the average position of shared substrings in neighbour sentences is closest to the end for the first hidden layer and moves towards the beginning of the sentence for the second and third hidden layer. This indicates a difference between the layers with regards to the timescale they represent. Whereas in the lowest layer information from the latest timesteps is most explanatory for the current activation vector, the timescale that is taken into account grows for the second and third hidden layer. 

[Example: pair of sentences for each layer
highlighted shared substrings, with positions aligned ]

\begin{table}[]
	\centering
	\begin{tabular}{cc}
		Layer 1 & 12.09 \\
		Layer 2 & 14.90 \\
		Layer 3 & 16.80
	\end{tabular}
	\caption{Average position of symbols in shared subsequences between nearest neighbour sentences}
\end{table}