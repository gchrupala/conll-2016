\section{Related work}
% kleine zin om te zeggen waar je het aan gaat relateren?

In language acquisition research, grounded learning of (sometimes combined) word learning and word segmentation is often studied in the cross-situational learning paradigm. %you should probably extend this somewhat & mention some experimental work. also that sentence is really ugly.
Two computational models that take the continuous nature of the speech signal into account, as well as incorporating visual information, are the Cross-channel Early Lexical Learning (CELL) model of \newcite{Roy2002113} and the more recent work of \newcite{rasanen2015joint}. The CELL model aims to discover words from continuous speech and learn their meaning, but is very limited with regards to visual input, which only consists of the shape of single objects in the visual context. More recently, \newcite{rasanen2015joint} propose a probabilistic joint model of word segmentation and meaning acquisition from raw speech and visual context. The visual context in this model consists of a collection of possible referents that are present in the environment, making it richer than the visual input in CELL, but also abstracting away further from the raw visual data. 

There is an extensive line of research in automatic image captioning (see \newcite{bernardi2016automatic} for a recent overview). Typically, image captioning models learn to recognize high-level image features and associate them with words. Inspired by both image captioning research and cross-situational human language acquisition, two recent Automatic Speech Recognition models learn to recognize word forms from visual data. In \newcite{synnaeve2014learning}, language input consists of single spoken words and visual data consists of image fragments, that the model learns to associate. \newcite{harwath2015deep} employ two convolutional neural networks, a visual object recognition model and a word recognition model, and an embedding alignment model that learns to map recognized words and objects into the same high-dimensional space. Although the object recognition works on the raw visual input, the speech signal is segmented into words before presenting it to the word recognition model. Whereas in \newcite{harwath2015deep} the visual input only plays a role at the utterance level, and not at the sub-word level, \newcite{synnaeve2014learning} only consider the (sub)-word level and do not look at the larger utterance context. 

Character level language models are described and interpreted in \newcite{hermans2013training} and \newcite{karpathy2015visualizing}. Both studies show that character-level deep recurrent neural networks are sensitive to long-range dependencies, such as keeping track of opening and closing parentheses over long stretches of text. \newcite{hermans2013training} describe the somewhat hierarchical organization that seems to emerge during training, with higher layers processing information over linger timescales. We will be using these findings to interpret the workings of our model, as it operates on the phonemic equivalent of characters, albeit in a multimodal setting. 

Whereas our study does not concern the continuous speech signal, we aim to take a step towards multimodal language learning from speech and images, by taking as input a symbolic phonemic transcription of full sentences paired with the images they describe. In contrast to \newcite{Roy2002113} and \newcite{rasanen2015joint}, the visual input to our model consists of high-level visual features, which means it contains ambiguity and noise. In contrast to \newcite{synnaeve2014learning} and  \newcite{harwath2015deep}, we consider full utterances rather than separate words. To our knowledge, there is no work yet on multimodal character-level language modelling with visual input. Our study is complementary to work on character level language modelling, as it presents a model of grounded language learning at the character level. Although the language input in this study is symbolic rather than perceptual, the learning problem we aim to solve is similar to that of a human language learner: the learner has to discover language structure as well as meaning, based on ambiguous and noisy data from another modality.  

\newcite{chrupala2015learning} simulate visually grounded human language learning with specific sensitivity to the noise and ambiguity in the visual domain. In an approach that is technically similar to recent work on neural image captioning in the computer vision and NLP domain, they model language learning as a task of predicting visual context given a sequence of words. While the visual input consists of a continuous representation, the language input consists of a sequence of words. The aim of this study is to take their approach one step further towards multimodal language learning from continuous visual and speech data. 