\section{Related work}
% kleine zin om te zeggen waar je het aan gaat relateren?

In language acquisition research, grounded learning of (sometimes combined) word learning and word segmentation is often studied in the cross-situational learning paradigm. %you should probably extend this somewhat & mention some experimental work. also that sentence is really ugly.
Two computational models that take the continuous nature of the speech signal into account, as well as incorporating visual information, are the Cross-channel Early Lexical Learning (CELL) model of \newcite{Roy2002113} and the more recent work of \newcite{rasanen2015joint}. The CELL model aims to discover words from continuous speech and learn their meaning, but is very limited with regards to visual input, which only consists of the shape of single objects in the visual context. \newcite{rasanen2015joint} propose a probabilistic joint model of word segmentation and meaning acquisition from raw speech and visual context. The visual context in this model consists of a collection of possible referents present in the environment, making it richer than the visual input in CELL, but also abstracting away further from the raw visual data. 

There is an extensive line of research in automatic image captioning (see \newcite{bernardi2016automatic} for a recent overview). Typically, image captioning models learn to recognize high-level image features and associate them with words. Inspired by both image captioning research and cross-situational human language acquisition, two recent Automatic Speech Recognition models learn to recognize word forms from visual data. In \newcite{synnaeve2014learning}, language input consists of single spoken words and visual data consists of image fragments, which the model learns to associate. \newcite{harwath2015deep} employ two convolutional neural networks, a visual object recognition model and a word recognition model, and an embedding alignment model that learns to map recognized words and objects into the same high-dimensional space. Although the object recognition works on the raw visual input, the speech signal is segmented into words before presenting it to the word recognition model. Whereas in \newcite{harwath2015deep} the visual input only plays a role at the utterance level, and not at the sub-word level, \newcite{synnaeve2014learning} only consider the (sub)-word level and do not look at the larger utterance context. 

Character-level language models are described and interpreted in \newcite{hermans2013training} and \newcite{karpathy2015visualizing}. Both studies show that character-level deep recurrent neural networks are sensitive to long-range dependencies: for example by keeping track of opening and closing parentheses over long stretches of text. \newcite{hermans2013training} describe the somewhat hierarchical organization that seems to emerge during training, with higher layers processing information over linger timescales. In our work we show related effects in the context of a model of visually-grounded language learning from unsegmented phonetic strings.

In the current study we use phonetic transcription of full sentences as a first step towards large-scale multimodal language learning from speech co-occurring with visual scenes. In contrast to \newcite{Roy2002113} and \newcite{rasanen2015joint}, the visual input to our model consists of high-level visual features, which means it contains ambiguity and noise. In contrast to \newcite{synnaeve2014learning} and  \newcite{harwath2015deep}, we consider full utterances rather than separate words. To our knowledge, there is no work yet on multimodal phoneme or character-level language modeling with visual input. Our study is complementary to work on character-level language modeling: while language models focus on {\it within}-text co-occurrence statistics as clues to language structure, in our setting the weight is on the correlations  {\it between} language and the visual scenes. Although the language input in this study is low-level-symbolic rather than perceptual, the learning problem we aim to solve is similar to that of a human language learner: the learner has to discover language structure as well as meaning, based on ambiguous and noisy data from another modality.  

\newcite{chrupala2015learning} simulate visually grounded human language learning with specific sensitivity to the noise and ambiguity in the visual domain. They model language learning as a task of predicting visual context given a sequence of words. While the visual input consists of a continuous representation, the language input consists of a sequence of words. The aim of this study is to take their approach one step further towards multimodal language learning from raw perceptual input. 

\newcite{Kdr2016RepresentationOL} develop techniques for understanding, interpretation and visualization of the representations of linguistic form and meaning in recurrent neural networks, and apply these to word-level models. In our work we share the goal of revealing the nature of emerging representations, but we do not assume words and word embeddings as their basic unit. Also, we are especially concerned with the emergence of a hierarchy of levels of representations in stacked recurrent networks.

