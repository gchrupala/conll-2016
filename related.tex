\section{Related work}
A large part of language acquisition consists of learning its structure, but in order to be able to communicate it is also of vital importance to learn the relation of words to entities in the world. % I don't like this sentence
Grounded lexical acquisition is often modeled as cross-situational learning, a process of rule-based \cite{siskind.96} or statistical inference \cite{fazly.etal.10csj, frank.etal.07} of word-to-referent mappings, based on the co-occurence of words and objects over multiple encounters. 
Cross-situational models typically work on word-level language input and symbolic representations of the context, whereas infants have to learn from continuous perceptual input. Recent experimental and computational studies have found that co-occurring visual information may help to learn word forms \cite{thiessen2010effects, Cunillera2010295, Glicksohn2013, Yurofsky2012statistical}. This suggests that acquisition of word form and meaning should be seen as interactive, rather than separate processes.

The Cross-channel Early Lexical Learning (CELL) model of \newcite{Roy2002113} and the more recent work of \newcite{rasanen2015joint} take into account the continuous nature of the speech signal, and incorporate visual information as well. The CELL model learns to discover words in continuous speech through co-occurence with their visual referent, but the visual input only consists of the shape of single objects, effectively bypassing referential uncertainty. \newcite{rasanen2015joint} propose a probabilistic joint model of word segmentation and meaning acquisition from raw speech and a set of possible referents that appear in the context. In both \newcite{Roy2002113} and \newcite{rasanen2015joint}, then, the visual context is considerably less noisy and ambiguous than that available to children.

There is an extensive line of artificial intelligence research on automatic image captioning (see \newcite{bernardi2016automatic} for a recent overview). Typically, image captioning models learn to recognize high-level image features and associate them with words. Inspired by both image captioning research and cross-situational human language acquisition, two recent automatic speech recognition models learn to recognize word forms from visual data. In \newcite{synnaeve2014learning}, language input consists of single spoken words and visual data consists of image fragments, which the model learns to associate. \newcite{harwath2015deep} employ two convolutional neural networks, a visual object recognition model and a word recognition model, and an embedding alignment model that learns to map recognized words and objects into the same high-dimensional space. Although the object recognition works on the raw visual input, the speech signal is segmented into words before presenting it to the word recognition model. Both \newcite{harwath2015deep} and \newcite{synnaeve2014learning} recognize words from pre-segmented speech, but the location of word boundaries is not available to language learning infants.

The character level has recently gained attention in various NLP applications. \newcite{ling2015finding} and \newcite{plank2016multilingual} use bidirectional LSTMs that read every word character-by-character and produce composed word vectors. While these word vectors have the advantage of encoding information about the surface form and can even be informative for unknown words, they can only be formed when word boundaries are known. % terrible no good very bad sentence. 
\newcite{chung2016character} present machine translation with character level output, but the \textit{input} consists of sub-word units as in \newcite{sennrich2015neural}. These units may correspond to whole words, morphemes, or characters, but are restricted to subsequences of individual words and never cross word boundaries. Character-level neural NLP \textit{without} explicit exploitation of word boundaries in the input is studied in some specific cases where fixed vocabularies are inherently problematic, e.g. with combined natural and programming language input \cite{chrupala2013text} or when specifically dealing with misspelled words in automatic writing feedback \cite{xie2016neural}. 

Character-level language models are described and interpreted in \newcite{hermans2013training} and \newcite{karpathy2015visualizing}. Both studies show that character-level deep recurrent neural networks are sensitive to long-range dependencies: for example by keeping track of opening and closing parentheses over long stretches of text. \newcite{hermans2013training} describe the hierarchical organization that seems to emerge during training, with higher layers processing information over longer timescales. In our work we show related effects in the context of a model of visually-grounded language learning from unsegmented phonetic strings. Even when they are not exploited explicitly, whitespaces and punctuation still provide implicit cues to the location of word boundaries. Our input data does not contain these cues.

In the current study we use phonetic transcription of full sentences as a first step towards large-scale multimodal language learning from speech co-occurring with visual scenes. In contrast to \newcite{Roy2002113} and \newcite{rasanen2015joint}, the visual input to our model consists of high-level visual features, which means it contains ambiguity and noise. In contrast to \newcite{synnaeve2014learning} and \newcite{harwath2015deep}, we consider full utterances rather than separate words. As \newcite{harwath2015deep} note, the learning task of a multimodal model becomes significantly more complicated when the language input consists of unsegmented speech. The absence of word boundaries in the input data is essential to our aim, and we choose to use unsegmented phoneme transcriptions rather than raw speech. Additionally, phonemic input data allows us to perform experiments on the encoding of linguistic knowledge as reported in section \ref{sec:experiments} without additional annotation.

To our knowledge, there is no work yet on multimodal phoneme or character-level language modeling with visual input. % multimodal and visual - double?
Our study is complementary to work on character-level language modeling: while language models focus on {\it within}-text co-occurrence statistics as clues to language structure, in our setting the weight is on the correlations  {\it between} language and the visual scenes. 
Although the language input in this study is low-level-symbolic rather than perceptual, the learning problem we aim to solve is similar to that of a human language learner: the learner has to discover language structure as well as meaning, based on ambiguous and noisy data from another modality. 

\newcite{chrupala2015learning} simulate visually grounded human language learning with specific sensitivity to the noise and ambiguity in the visual domain. They model language learning as a task of predicting visual context given a sequence of words. While the visual input consists of a continuous representation, the language input consists of a sequence of words. The aim of this study is to take their approach one step further towards multimodal language learning from raw perceptual input. 

\newcite{Kdr2016RepresentationOL} develop techniques for understanding, interpretation and visualization of the representations of linguistic form and meaning in recurrent neural networks, and apply these to word-level models. In our work we share the goal of revealing the nature of emerging representations, but we do not assume words and word embeddings as their basic unit. Also, we are especially concerned with the emergence of a hierarchy of levels of representations in stacked recurrent networks.