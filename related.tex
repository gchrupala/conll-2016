\section{Related work}
% kleine zin om te zeggen waar je het aan gaat relateren?

In language acquisition research, grounded learning of (sometimes combined) word learning and word segmentation is often studied in the cross-situational learning paradigm. %you should probably extend this somewhat & mention some experimental work. also that sentence is really ugly.
Two computational models that take the continuous nature of the speech signal into account, as well as incorporating visual information, are the Cross-channel Early Lexical Learning (CELL) model of \newcite{Roy2002113} and the more recent work of \newcite{rasanen2015joint}. The CELL model aims to discover words from continuous speech and learn their meaning, but is very limited with regards to visual input, which only consists of the shape of single objects in the visual context. More recently, \newcite{rasanen2015joint} propose a probabilistic joint model of word segmentation and meaning acquisition from raw speech and visual context. The visual context in this model consists of a collection of possible referents that are present in the environment, making it richer than the visual input in CELL, but also abstracting away further from the raw visual data. % how does this relate to our study? Is it not similar to speech recognition trained on visual data?
Another related field is in the tradition of Automatic Speech Recognition (ASR). Whereas ASR models are typically trained on speech recordings with aligned textual transcriptions, there has recently been work on learning to recognize speech from accompanying visual data.
\newcite{harwath2015deep} employ two convolutional neural networks, a visual object recognition model and a word recognition model, and an embedding alignment model that learns to map recognized words and objects into the same high-dimensional space. Although the object recognition works on the raw visual input, the speech signal is segmented into words before presenting it to the word recognition model. In the work of \newcite{synnaeve2014learning} the language input consists of single spoken content words that appear in the captions accompanying the images. %chekc I'm not sure if they really were single words
Whereas in \newcite{harwath2015deep} the visual input only plays a role at the utterance level, and not at the sub-word level, \newcite{synnaeve2014learning} only consider the (sub)-word level an do not look at the larger utterance context. Whereas our study does not concern the continuous speech signal, we aim to take a step towards multimodal language learning from speech and images, by taking as input a symbolic phonetic transcription of a full sentence paired with the image it describes.

% Apart from language acquisition research, this study is also related to current work in the cross-over of computer vision and natural language processing on image captioning. [something on image captioning (problem is that you now have too much to choose from), multimodal translation (Elliot Frank Hasler)… ….] % perhaps revfer to overview paper:
% http://arxiv.org/pdf/1505.01809.pdf or http://arxiv.org/pdf/1601.03896.pdf

Character level language models are described and interpreted in \newcite{hermans2013training} and \newcite{karpathy2015visualizing}. Both studies show that character-level deep recurrent neural networks are sensible to long-range dependencies, such as keeping track of opening and closing parentheses over long stretches of text. \newcite{hermans2013training} describe the somewhat hierarchical organization that seems to emerge during training, with higher layers processing information over linger timescales. We will be using these findings to interpret the workings of our model, as it operates on the phonemic equivalent of characters, albeit in a multimodal setting. To our knowledge, there is no work yet on multimodal character-level language modelling with visual input. % dangerous


\newcite{chrupala2015learning} simulate visually grounded human language learning with specific sensitivity to the noise and ambiguity in the visual domain. In an approach that is technically similar to recent work on neural image captioning in the computer vision and NLP domain, they model language learning as a task of predicting visual context given a sequence of words. While the visual input consists of a continuous representation, the language input consists of a sequence of words. The aim of this study is to take their approach one step further towards multimodal language learning from continuous visual and speech data. While our language data is still symbolic, it consists of the smallest linguistic unit.

% some work on phoneme/charaxcter level : http://arxiv.org/pdf/1506.02078v2.pdf ... perhaps https://arxiv.org/pdf/1508.06615v4.pdf ? also schrauwens & hermans. more linguistically interesting; composed word representations: http://arxiv.org/pdf/1508.02096v1.pdf To the best of our knowledge, however, 